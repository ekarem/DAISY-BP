---
HIGH
---
DAISY

*functionality
 - demonstrate;

 - performance, need a set of graphs/tables with the standard resolutions (320x240,640x480,etc...);
   - a table with time measurements per operation along those resolutions + speed(hz)

 - get graph showing difference with and without overlap

 - look at SIFT on GPU papers to see verification or benchmarking metrics/methods

 - get a pair of images with different perspectives/lighting and show descriptor do its thing with a few matches (these images can represent the capability/purpose of the descriptor)

 (**big bracket** ie leave for another time)
   (((- other GPU implementations think they are fast with feature detection but in a dense computation
      they would slug to very slow speeds... apart from extrapolating their performance and comparing
      what they ''would'' perform if they were used densely another way to compare could be;
      1) write a kernel function on GPU to compute descriptors on-the-fly from the un-transposed data (transA but not transB)
          and that should be pretty fast, and then an arbitrary number of descriptors could be requested to demonstrate the speed
      2) have the ability to give a list of image locations to compute descriptors for to transB kernel)))
  BUT INSTEAD;
  - using the simple method just now tested of timing the global reads and then including the actual transposition + writes
    and seeing how long the reads take we can see clearly that the majority of the time the kernel takes to run depends directly on 
    the number of output descriptors and each descriptor takes time 0.000000145 seconds which means that when in the region of low
    numbers of features such as <=100000 (100k being huge for any other implementation found) this time keeps at (assuming linearly) <=14.5 ms; 
    which is way way faster than any other detector+descriptor extractor found for the GPU or not, even for SURF. (set of bookmarks on laptop)

  -->Tested the impact of the local read + global write in the transposeDaisy kernel. The major times are held by the global write, a quarter of the slowness is due to misaligned addresses due to not all the petals that are being written are aligned to 64-bytes. But new GPUs by nvidia managed to make coalesced writes even with misaligned addresses as long as they are in the same 64-byte/128-byte segment and by consecutive workers. Some also dont care if the workers are writing consecutively or not..
  -->The times for the transposeDaisy kernel sub-operations are;
     - 2ms for the global read of (512+32)x(512+32)x8x3 floats coalesced read 3551232000 (13GB/sec)
     - 25ms for global write of 512x512x200 floats in a coalesced(?) fashion  2097152000 (8GB/sec)
     - 35ms for global write of 512x512x200 floats in a consecutive range of addresses but misaligned (5.6GB/sec)

  - still need to find SIFT/SURF dense computation times done on the GPU though.

*downsampling with higher sigmas
 - think this through
 - how the blurr + downsample will happen
 - how the data fetching of downsampled data will happen
 - try it full speed
 + ideally the above should be written for variable blurr/downsample rate
 + consider the genericness of the implementation wrt. sigmas and number of gradients (as much as possible) 

 - properly measure CPU version with 1 core and 8 cores in the same range of inputs as GPU (done)
 - re-measure everything with standard deviation included (done)

 - test program on nvidia card WITH CKE and CC overlap!!!!! jeremy's card opportunity no miss! 
    run the regular set of inputs there with the existing code and pop out the measures! 
    (in theory should achieve upto half the time in transB+transfer and maybe a tiny bit faster 
      if the CKE helps between the smoothings!)

 - validate output;
   - percentage of max difference between 2 CPUs and between 2 GPUs (done)
   - percentage of difference between CPU and GPU descriptors in one image (done)
   - real-time rigid body correspondence in stereo? (rotational invariance... mmmm!)

*implementation
 - target 512x512 computation for speed up
 - require group size in all kernels
 - simplify final transposition? (medium)


REPORT
 - lay out the structure
 - start putting the technical stuff together
   performance:
   - big drops in with-transfer speeds -> make a table with overlapping steps and times to demonstrate overlap and impact of it and why the speedup increases with more and more blocks (impact of the remainder transfer that doesn't overlap reduces) -> drop after 1024x1024 is much smaller than drop after 512x512
   - relative std of times reach an isolated max peak (with low number of points) of ~3% but even that is trivial
     in reality (about 5ms) for that computation that takes about 120ms
   - highlight that this is DENSE computation and therefore number of pixels is equivalent to number of features computed,
     the comparison to other implementations on the GPU of SIFT or any other then becomes a trivial win in terms of time, even with the transfer to CPU

---
MEDIUM
---
test data
record performance test results with visual profiler
---
LOW
---
manage sectioning of input images into 1024x1024 sections for larger images
BP implementation
*for report
  - make incremental optimisation kernels (convolution)
  - correctness testing (conv, gradients)
(*belief propagation matching - heavy reading papers/code understand bp - understood up until a point, read sift flow code or other to understand properly, wait until DAISY is done)
(-choose objective function for BP - vaguely done, leave for later)
---
WAIT
---
---
DONE
---
RGB or grayscale images? - grayscale
*convolutions should rely only on x64 padding not x128, some are on x128 now
GIT setup
GIT setup online github
**lit review
*DAISY implementation
  - tested transposition by storing source indeces
software license? - open source
