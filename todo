---
HIGH
---
DAISY

*functionality
 - properly measure CPU version with 1 core and 8 cores in the same range of inputs as GPU (done)
 - re-measure everything with standard deviation included (done)
 - performance, need a set of graphs/tables with the standard resolutions (320x240,640x480,etc...);
   - a table with time measurements per operation along those resolutions + speed(hz)
 - validate output;
   - percentage of max difference between 2 CPUs and between 2 GPUs (done)
   - percentage of difference between CPU and GPU descriptors in one image (done)
   - look at SIFT on GPU papers to see verification or benchmarking metrics/methods
 - demonstrate;
   - get a few pairs of images with different perspectives/lighting and show descriptor do its thing with a few matches (these images can represent the capability/purpose of the descriptor)
   - real-time rigid body correspondence in stereo? (rotational invariance... mmmm!)

*implementation
 - target 512x512 computation for speed up
 - require group size in all kernels
 - simplify final transposition? (medium)

*downsampling with higher sigmas
 - think this through
 - how the blurr + downsample will happen
 - how the data fetching of downsampled data will happen
 - try it full speed
 + ideally the above should be written for variable blurr/downsample rate
 + consider the genericness of the implementation wrt. sigmas and number of gradients (as much as possible) 

REPORT
 - lay out the structure
 - start putting the technical stuff together
   performance:
   - big drops in with-transfer speeds -> make a table with overlapping steps and times to demonstrate overlap and impact of it and why the speedup increases with more and more blocks (impact of the remainder transfer that doesn't overlap reduces) -> drop after 1024x1024 is much smaller than drop after 512x512
   - relative std of times reach an isolated max peak (with low number of points) of ~3% but even that is trivial
     in reality (about 5ms) for that computation that takes about 120ms
   - highlight that this is DENSE computation and therefore number of pixels is equivalent to number of features computed,
     the comparison to other implementations on the GPU of SIFT or any other then becomes a trivial win in terms of time, even with the transfer to CPU

---
MEDIUM
---
test data
record performance test results with visual profiler
---
LOW
---
manage sectioning of input images into 1024x1024 sections for larger images
BP implementation
*for report
  - make incremental optimisation kernels (convolution)
  - correctness testing (conv, gradients)
(*belief propagation matching - heavy reading papers/code understand bp - understood up until a point, read sift flow code or other to understand properly, wait until DAISY is done)
(-choose objective function for BP - vaguely done, leave for later)
---
WAIT
---
---
DONE
---
RGB or grayscale images? - grayscale
*convolutions should rely only on x64 padding not x128, some are on x128 now
GIT setup
GIT setup online github
**lit review
*DAISY implementation
  - tested transposition by storing source indeces
software license? - open source
